{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMSfEzU6PS/6HLojewsa7cL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/athensclub/A10-Library/blob/master/testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptF-auPQbHqW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, Concatenate, Flatten,LSTM\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqF9rbiFbMWT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "characters = 'กขฃคฅฆงจฉชซฌญฎฏฐฑฒณดตถทธนบปผฝพฟภมยรฤลฦวศษสหฬอฮฯะัาำิีึืฺุู฿เแโใไๅๆ็่้๊๋์ํ๎๏๐๑๒๓๔๕๖๗๘๙abcdefghijklmnopqrstuvwxyz\"\\'0123456789,.!?/\\\\:;%()[]{}+_-*@#><=^$& \\t\\n'\n",
        "char_encode = {}\n",
        "char_decode = {}\n",
        "i = 1\n",
        "for c in characters:\n",
        "  char_encode[c] = i;\n",
        "  char_decode[i] = c;\n",
        "  i += 1\n",
        " \n",
        "def encode(data):\n",
        "  encoded = []\n",
        "  data = data.lower()\n",
        "  for c in data:\n",
        "    encoded.append(char_encode[c])\n",
        "  return encoded\n",
        "  \n",
        "def decode(data):\n",
        "  decoded = ''\n",
        "  for c in data:\n",
        "    if c != 0:\n",
        "      decoded = decoded + char_decode[c]\n",
        "  return decoded\n",
        " \n",
        "def split_text_data(encoded,length=256):\n",
        "  splitted = []\n",
        "  chunk = []\n",
        "  temp = []\n",
        "  for i in range(len(encoded)):\n",
        "    c = encoded[i]\n",
        "    temp.append(c)\n",
        "    if c == char_encode[' ']:\n",
        "      if len(temp) > 0:\n",
        "        if len(temp) + len(chunk) < length:\n",
        "          chunk.extend(temp)\n",
        "          temp = []\n",
        "        else:\n",
        "          splitted.append(chunk)\n",
        "          chunk = []\n",
        "          chunk.extend(temp)\n",
        "          temp = []\n",
        "  #cleaning leftovers\n",
        "  if len(temp) > 0:\n",
        "    if len(temp) + len(chunk) < length:\n",
        "      chunk.extend(temp)\n",
        "    else:\n",
        "      splitted.append(chunk)\n",
        "      chunk.extend(temp)\n",
        "  if len(chunk) > 0:\n",
        "    splitted.append(chunk)\n",
        "  return splitted\n",
        "  \n",
        "def tokenize(text):\n",
        "  tokenized = []\n",
        "  splitted = split_text_data(encode(text))\n",
        "  for chunk in splitted:\n",
        "    temp = []\n",
        "    before = []\n",
        "    after = chunk.copy()\n",
        "    for c in chunk:\n",
        "      temp.append(c)\n",
        "      after.pop(0)\n",
        "      pred = model.predict([sequence.pad_sequences([before],256),sequence.pad_sequences([temp],256),sequence.pad_sequences([after],256)])[0]\n",
        "      if pred > 0.5:\n",
        "        tokenized.append(decode(temp))\n",
        "        before.extend(temp)\n",
        "        temp = []\n",
        "    #cleaning leftovers\n",
        "    if len(temp) > 0:\n",
        "      tokenized.append(decode(temp))\n",
        "  return tokenized\n",
        "  \n",
        "model = load_model('new_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHkv24eZbQF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenize('การตัดคำภาษาไทย')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}